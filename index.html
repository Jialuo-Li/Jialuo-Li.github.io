<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Jialuo Li | Êùé‰Ω≥ÁÉô</title>

    <meta name="author" content="Jialuo Li">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/icon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Jialuo Li
                </p>
                <p>
                  I'm a senior undergraduate student majoring Computer Science and Technology at <a href="https://iiis.tsinghua.edu.cn/en/">IIIS</a>, 
                  <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a> (a.k.a <a href="https://iiis.tsinghua.edu.cn/en/yaoclass/">Yao Class</a>, , directed by the Turing Award Laureate <a href="https://iiis.tsinghua.edu.cn/yao/">Andrew Chi-Chih Yao</a>). 
                  Currently I am a research intern at <a href="https://www.nyu.edu/">NYU</a> advised by Prof. <a href="https://www.sainingxie.com/">Saining Xie</a>. 
                  In addition, I am privileged to work with Prof. <a href="https://ericyi.github.io/">Li Yi</a>.
                </p>
                <p style="text-align:center">
                  <a href="mailto:lijialao21@mails.tsinghua.edu.cn">Email</a> &nbsp;/&nbsp;
                  <a href="data/CVJialuo.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=xl9hSggAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://x.com/JialuoLi1007">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/Jialuo-Li">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/Jialuo.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/Jialuo.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <h2>Research</h2>

            <tr>
              <td style="width:100%;vertical-align:middle">
                <p>
                  Humans exhibit an extraordinary capacity to integrate information from multiple sensory modalities, such as vision, auditory, and tactile inputs, to navigate and interpret their environments with remarkable efficiency. This multimodal integration leverages the complementary strengths of each sensory channel, facilitating a coherent and comprehensive understanding of complex surroundings. Inspired by this cognitive prowess, my long-term objective is to <strong>develop AI systems that emulate human-like multimodal synthesis, thereby enhancing robustness and adaptability in both generative and grounding tasks.</strong>
                </p>
              </td>
            </tr>
          </tbody></table>
          <br>

          <table style="width:95%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <h2>News</h2>

            <tr>
              <td style="width:100%;vertical-align:middle">
                <p>
                  <li><strong>2025-02:</strong> ‚ú® Our paper <a href="https://jialuo-li.github.io/">Scibench</a> was accepted by CVPR 2025!</li>
                  <li><strong>2024-09:</strong> I will join MSRA for an internship with Dr. <a href="https://www.microsoft.com/en-us/research/people/libin/">Bin Li</a>! </li>
                  <!-- <li><strong>2024-09:</strong> ‚ú® Our research project about reflection anomaly in generative model is now availiable!</li> -->
                  <li><strong>2024-07:</strong> ‚ú® Our paper <a href="https://arxiv.org/abs/2404.01343">CHOPS</a> was accepted by COLM 2024!</li>
                  <li><strong>2024-02:</strong> I will start my internship in NYU Courant advised by Prof. <a href="https://www.sainingxie.com/" target="_blank">Saining Xie</a>. Looking forward to working with Saining in New York!</li>
                  <!-- <li><strong>2024-01:</strong> ‚ú® Our graphics course project is now available! Feel free to explore and try it out <a href="https://github.com/ziyuyuyuyu1/ACG-Project">here</a>.</li> -->
                </p>
              </td>
            </tr>
          </tbody></table>
          <br>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <h2>Publications</h2>
            (* for equal contribution)
            <tr>
              <td style="padding:20px;width:40%;vertical-align:middle">
                <a href="YOUR_LINK_HERE" style="display:inline-block;"> 
                    <img src="images/Scibench.png" alt="Scibench" width="200" style="border-style: none; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.3); transition: filter 0.3s ease;">
                </a>
              </td>
              <td width="60%" valign="middle">
                <a href="https://jialuo-li.github.io">
                  <span class="papertitle"> SCIBENCH: Addressing Scientific Illusions in Image Synthesis
                  </span>
                </a>
                <br>
                <strong>Jialuo Li</strong>, 
                <a href="https://rese1f.github.io/">Wenhao Chai</a>, 
                <a href="https://zeyofu.github.io/">Xingyu Fu</a>, 
                <a href="https://github.com/cjxh21">Haiyang Xu</a>, 
                <a href="https://www.sainingxie.com/">Saining Xie</a>
                <br>
                <em>CVPR 2025 </em>
                <br>
                <a href="https://jialuo-li.github.io">Coming Soon</a>
                <!-- /
                <a href="https://arxiv.org/abs/2404.01343">arXiv</a> -->
                <p></p>
                <p>We introduce SCISCORE, an end-to-end reward model that enhances the scientific realism and consistency of generative models by integrating scientific knowledge through a two-stage training framework. Validated using the SCIBENCH, SCISCORE achieves human-level performance and significantly improves the evaluation of scientifically accurate image synthesis.</p>
              </td>
            </tr>
<!--             
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/CHOPS.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2404.01343">
                  <span class="papertitle"> CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs
                  </span>
                </a>
                <br>
                <a href="https://github.com/JingzheShi">Jingzhe Shi</a>, 
                <strong>Jialuo Li</strong>, 
                <a href="https://github.com/Aquahorse">Qinwei Ma</a>, 
                <a href="https://github.com/Faded-Nebula">Zaiwen Yang</a>, 
                <a href="https://github.com/cjxh21">Huan Ma</a>, 
                <a href="https://scholar.google.com/citations?user=DOyVxx0AAAAJ&">Lei Li</a>
                <br>
                <em>COLM 2024 </em>
                <br>
                <a href="https://github.com/JingzheShi/CHOPS">Project Page</a>
                /
                <a href="https://arxiv.org/abs/2404.01343">arXiv</a>
                <p></p>
                <p>We propose CHOPS, an LLM agent designed to efficiently access user information, interact with existing systems, and provide accurate, safe responses by leveraging a combination of small and large LLMs. Validated using the CPHOS-dataset, CHOPS demonstrates its potential to enhance or replace human customer service.</p>
              </td>
            </tr> -->

            <!-- <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='nuvo_image'><video  width=100% muted autoplay loop>
                  <source src="images/nuvo.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/nuvo.jpg' width=100%>
                </div>
                <script type="text/javascript">
                  function nuvo_start() {
                    document.getElementById('nuvo_image').style.opacity = "1";
                  }

                  function nuvo_stop() {
                    document.getElementById('nuvo_image').style.opacity = "0";
                  }
                  nuvo_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://pratulsrinivasan.github.io/nuvo/">
                  <span class="papertitle">Nuvo: Neural UV Mapping for Unruly 3D Representations</span>
                </a>
                <br>
                <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
                <a href="http://stephangarbin.com/">Stephan J. Garbin</a>,
                <a href="https://dorverbin.github.io/">Dor Verbin</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="https://bmild.github.io/">Ben Mildenhall</a>
                <br>
                <em>ECCV</em>, 2024
                <br>
                <a href="https://pratulsrinivasan.github.io/nuvo/">project page</a>
                /
                <a href="https://www.youtube.com/watch?v=hmJiOSTDQZI">video</a>
                /
                <a href="http://arxiv.org/abs/2312.05283">arXiv</a>
                <p></p>
                <p>
                Neural fields let you recover editable UV mappings for the challenging geometries produced by NeRF-like models.
                </p>
              </td>
            </tr>


            <tr onmouseout="cat3d_stop()" onmouseover="cat3d_start()" bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='cat3d_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/cat3d.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/cat3d.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function cat3d_start() {
                    document.getElementById('cat3d_image').style.opacity = "1";
                  }

                  function cat3d_stop() {
                    document.getElementById('cat3d_image').style.opacity = "0";
                  }
                  cat3d_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://cat3d.github.io/">
                <span class="papertitle">CAT3D: Create Anything in 3D with Multi-View Diffusion Models</span>
                </a>
                <br>
                <a href="https://ruiqigao.github.io/">Ruiqi Gao</a>*,
                <a href="https://holynski.org/">Aleksander Holynski</a>*, 
                <a href="https://henzler.github.io/">Philipp Henzler</a>,
                <a href="https://github.com/ArthurBrussee">Arthur Brussee</a>, 
                <a href="http://ricardomartinbrualla.com/">Ricardo Martin Brualla</a>, 
                <a href="https://pratulsrinivasan.github.io/">Pratul P. Srinivasan</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="https://poolio.github.io/">Ben Poole</a>*

                <br>
                <em>arXiv</em>, 2024
                <br>
                <a href="https://cat3d.github.io/">project page</a>
                /
                <a href="https://arxiv.org/abs/2405.10314">arXiv</a>
                <p></p>
                <p>
                A single model built around diffusion and NeRF that does text-to-3D, image-to-3D, and few-view reconstruction, trains in 1 minute, and renders at 60FPS in a browser.
                </p>
              </td>
            </tr> -->

          </tbody></table>

          <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <h2>Projects</h2>
            (* for equal contribution)
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/reflection.png" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ziyuyuyuyu1.github.io/ACG-UI/">
                <span class="papertitle"> Advancing Diffusion Models for Enhanced Mirror and Reflection Synthesis: A
                  Dual Approach Using Supervised FineTuning and Reinforcement Learning
                </span>
              </a>
              <br>
              <strong>Jialuo Li</strong>*, 
              <a href="https://github.com/ziyuyuyuyu1">Ziru Huang</a>*
              <br>
              <a href="https://huggingface.co/Summer-Project">Dataset</a>
              /
              <a href="/data/reflection_report.pdf">Report</a>
              <p></p>
              <p>We proposes enhancing the realism of mirrors and reflections in diffusion models through supervised fine-tuning (SFT) on a curated dataset and reinforcement learning (RL) guided by a custom reward model. While SFT provides modest improvements, the RL approach significantly boosts the accuracy and realism of generated reflections.</p>
            </td>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/acg_project.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://ziyuyuyuyu1.github.io/ACG-UI/">
                  <span class="papertitle"> Generative 3D Mesh Modeling with Text-to-Texture Generator
                  </span>
                </a>
                <br>
                <strong>Jialuo Li</strong>*, 
                <a href="https://github.com/ziyuyuyuyu1">Ziru Huang</a>*
                <br>
                <a href="https://ziyuyuyuyu1.github.io/ACG-UI/">Project Page</a>
                /
                <a href="https://github.com/ziyuyuyuyu1/ACG-Project">Code</a>
                /
                <a href="/data/ACG_pre.pdf">Presentation</a>
                /
                <a href="/data/ACG_Final_Report.pdf">Report</a>
                <p></p>
                <p>Our project extends <a href="https://arxiv.org/abs/2303.08133">MeshDiffusion</a> by incorporating class conditioning for 3D mesh generation and using a pre-trained 2D diffusion model to align textures with textual descriptions, effectively generates and textures category-specific 3D meshes.</p>
              </td>
            </tr>
          </tbody></table> -->

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <h2>Experience</h2>
            <br>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/MSRA_logo.png", width="120"></td>
              <td width="75%" valign="center">
                <strong><a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/" target="_blank"><span class="papertitle">Microsoft Research Lab - Asia</span> </a></strong>
                <br> <em>2024.09 - Present</em><br>  <strong>Research Intern</strong>
                <br> Research Advisor: Dr. <a href="https://www.microsoft.com/en-us/research/people/libin/" target="_blank">Bin Li</a>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/nyu_logo.png", width="120"></td>
              <td width="75%" valign="center">
                <strong><a href="https://www.nyu.edu/" target="_blank"><span class="papertitle">New York University</span> </a></strong>
                <br> <em>2024.02 - Present</em><br>  <strong>Research Intern</strong>
                <br> Research Advisor: Prof. <a href="https://www.sainingxie.com/" target="_blank">Saining Xie</a>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/Tsinghua_logo.png", width="120"></td>
              <td width="75%" valign="center">
                <strong><a href="https://www.tsinghua.edu.cn/" target="_blank"><span class="papertitle">Tsinghua University</span> </a></strong>
                <br> <em>2021.09 - Present</em><br>  <strong>Undergraduate Student</strong>
                <br> Research Advisor: Prof. <a href="https://ericyi.github.io/">Li Yi</a>.
              </td>
            </tr>
          
          </tbody></table>
          <br>

          <table style="width:95%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <h2>Honors and Awards</h2>
            <tr>
              <td style="width:100%;vertical-align:middle">
                <p>
                  <li><strong>2023:</strong> Outstanding Scholarship in Social Work from Tsinghua University.</li>
                </p>
                <p>
                  <li><strong>2022:</strong> Mr. and Mrs. Wong Yi-Chung Award, Friends of Tsinghua University.</li>
                </p>
                <p>
                  <li><strong>2022:</strong> Outstanding Scholarship in Social Work from Tsinghua University.</li>
                </p>
                <p>
                  <li><strong>2021:</strong> Member of the Chinese team of the 21st Asian Physics Olympiad (<a href="http://asianphysicsolympiad.org/">APhO</a>).</li>
                </p>
                <p>
                  <li><strong>2020:</strong> Gold Medalist üèÖ in the 37th Chinese Physics Olympiad (<a href="https://cpho.pku.edu.cn/info/1070/1207.htm">CPhO</a>), <strong>ranking tenth nationwide</strong>.</li>
                </p>
              </td>
            </tr>

          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  This homepage is designed based on <a href="http://jonbarron.info/">Jon Barron</a>'s homepage and deployed on <a href="https://pages.github.com/">GitHub Pages</a>. Last updated: Mar 03, 2025.
                  <br> ¬© 2025 Jialuo Li
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
